<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Yiwei Lyu</title> <meta name="author" content="Yiwei Lyu"/> <meta name="description" content="Yiwei Lyu's Personal Website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://lvyiwei1.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yiwei</span> Lyu </h1> <p class="desc"><a href="#">CSE Ph.D. student at University of Michigan</a>. yiweilyu at umich.edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/portrait2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/portrait2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/portrait2-1400.webp"></source> <img src="/assets/img/portrait2.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="portrait2.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>3856 Bob Betty and Beyster Building</p> <p>2260 Hayward Street</p> <p>Ann Arbor, MI, 48105</p> </div> </div> <div class="clearfix"> <p>I am currently a Ph.D. student in Computer Science and Engineering at University of Michigan, co-advised by <a href="https://web.eecs.umich.edu/~honglak/" target="_blank" rel="noopener noreferrer">Dr. Honglak Lee</a> and <a href="https://medicine.umich.edu/dept/neurosurgery/todd-hollon-md" target="_blank" rel="noopener noreferrer">Dr. Todd Hollon</a>. Previously I received a B.S. in Computer Science and a M.S. in Machine Learning at Carnegie Mellon University, where I was advised by <a href="https://www.cs.cmu.edu/~pliang/" target="_blank" rel="noopener noreferrer">Paul Liang</a> and <a href="https://www.cs.cmu.edu/~morency/" target="_blank" rel="noopener noreferrer">Dr. Louis-Philippe Morency</a>.</p> <p>My research interests include multimodal machine learning, computer vision and natural language processing, as well as their applications in the real world, such as in healthcare.</p> <p>During my undergraduate and master’s, I have spent three summers doing research, supported by CMU <a href="https://www.cmu.edu/uro/sura/" target="_blank" rel="noopener noreferrer">Summer Undergraduate Research Apprenticeship</a> (2018), <a href="https://www.cmu.edu/uro/summer%20research%20fellowships/SURF/" target="_blank" rel="noopener noreferrer">Summer Undergraduate Research Fellowship</a> (2020), and Research Intern at <a href="http://multicomp.cs.cmu.edu/" target="_blank" rel="noopener noreferrer">CMU MultiComp Lab</a> (2021), and have worked as an undergraduate/graduate research assistant at <a href="https://squareslab.github.io/" target="_blank" rel="noopener noreferrer">CMU SquaresLab</a> and <a href="http://multicomp.cs.cmu.edu/" target="_blank" rel="noopener noreferrer">CMU MultiComp Lab</a>. I have also been a software engineer intern at Pinterest in the summer of 2019, and a teaching assistant for CMU 15-210 for 4 semesters.</p> <p>I have received Honorable Mention in <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/" target="_blank" rel="noopener noreferrer">CRA Outstanding Undergraduate Researcher Award</a> 2021, and I ranked 107th in <a href="https://www.maa.org/math-competitions/putnam-competition" target="_blank" rel="noopener noreferrer">William Lowell Putnam Competition</a> in 2017.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Dec 9, 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2403.13680" target="_blank" rel="noopener noreferrer">Step-Calibrated Diffusion for Biomedical Optical Image Restoration</a> is accepted at AAAI 2025! </td> </tr> <tr> <th scope="row">Oct 10, 2024</th> <td> Our paper <a href="https://arxiv.org/pdf/2305.19512" target="_blank" rel="noopener noreferrer">Code Models are Zero-shot Precondition Reasoner</a> is accepted at FITML Workshop at NeurIPS 2024! </td> </tr> <tr> <th scope="row">Mar 13, 2024</th> <td> Our paper <a href="https://arxiv.org/pdf/2311.09601.pdf" target="_blank" rel="noopener noreferrer">Code Models are Zero-shot Precondition Reasoners</a> is accepted at NAACL 2024! </td> </tr> <tr> <th scope="row">Oct 7, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2312.04668" target="_blank" rel="noopener noreferrer">TOD-Flow: Modeling the Structure of Task-Oriented Dialogues</a> is accepted at EMNLP 2023! </td> </tr> <tr> <th scope="row">Aug 31, 2023</th> <td> I will be doing a part-time internship at LG AI Research in Ann Arbor from September to December 2023! </td> </tr> <tr> <th scope="row">Jul 14, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2305.19512" target="_blank" rel="noopener noreferrer">Fine-grained Text Style Transfer with Diffusion-Based Language Models</a> won Best Paper Award at Repl4NLP Workshop at ACL 2023! </td> </tr> <tr> <th scope="row">May 24, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2305.19512" target="_blank" rel="noopener noreferrer">Fine-grained Text Style Transfer with Diffusion-Based Language Models</a> is accepted at Repl4NLP Workshop at ACL 2023! </td> </tr> <tr> <th scope="row">Apr 24, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2203.01311" target="_blank" rel="noopener noreferrer">HighMMT: Quantifying Modality and Interaction Heterogeneity for High-Modality Representation Learning</a> is accepted by TMLR! </td> </tr> <tr> <th scope="row">Jan 20, 2023</th> <td> Check out our new accepted paper <a href="https://arxiv.org/abs/2207.00056" target="_blank" rel="noopener noreferrer">MultiViz: Towards Visualizing and Understanding Multimodal Models</a> at ICLR 2023! </td> </tr> <tr> <th scope="row">Nov 20, 2022</th> <td> Check out our new accepted papers <a href="https://arxiv.org/pdf/2211.05750.pdf" target="_blank" rel="noopener noreferrer">Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control</a> and <a href="https://andy-xingbowang.com/papers/multiviz_nips_hill2022.pdf" target="_blank" rel="noopener noreferrer">MULTIVIZ: Towards Visualizing and Understanding Multimodal Models</a> at NeurIPS 2022 HILL Workshop! </td> </tr> <tr> <th scope="row">Aug 29, 2022</th> <td> Starting today as a Ph.D. student at University of Michigan! </td> </tr> <tr> <th scope="row">May 10, 2022</th> <td> Our paper <a href="https://arxiv.org/abs/2211.05750" target="_blank" rel="noopener noreferrer">Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control</a> is accepted at ACL Findings 2023! </td> </tr> <tr> <th scope="row">Apr 20, 2022</th> <td> Check out our new accepted paper <a href="https://arxiv.org/pdf/2203.02013.pdf" target="_blank" rel="noopener noreferrer">DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations</a> at AIES 2022! </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="lyu2024step" class="col-sm-8"> <div class="title">Step-calibrated diffusion for biomedical optical image restoration</div> <div class="author"> <em>Yiwei Lyu</em>, Sung Jik Cha, Cheng Jiang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Asadur Chowdury, Xinhai Hou, Edward Harake, Akhil Kondepudi, Christian Freudiger, Honglak Lee, Todd C Hollon' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In AAAI 2025</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2403.13680" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/MLNeurosurg/restorative_step-calibrated_diffusion" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>High-quality, high-resolution medical imaging is essential for clinical care. Raman-based biomedical optical imaging uses non-ionizing infrared radiation to evaluate human tissues in real time and is used for early cancer detection, brain tumor diagnosis, and intraoperative tissue analysis. Unfortunately, optical imaging is vulnerable to image degradation due to laser scattering and absorption, which can result in diagnostic errors and misguided treatment. Restoration of optical images is a challenging computer vision task because the sources of image degradation are multi-factorial, stochastic, and tissue-dependent, preventing a straightforward method to obtain paired low-quality/high-quality data. Here, we present Restorative Step-Calibrated Diffusion (RSCD), an unpaired diffusion-based image restoration method that uses a step calibrator model to dynamically determine the number of steps required to complete the reverse diffusion process for image restoration. RSCD outperforms other widely used unpaired image restoration methods on both image quality and perceptual evaluation metrics for restoring optical images. Medical imaging experts consistently prefer images restored using RSCD in blinded comparison experiments and report minimal to no hallucinations. Finally, we show that RSCD improves performance on downstream clinical imaging tasks, including automated brain tumor diagnosis and deep tissue imaging.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="dime" class="col-sm-8"> <div class="title">DIME: Fine-Grained Interpretations of Multimodal Models via Disentangled Local Explanations</div> <div class="author"> <em>Yiwei Lyu</em>, Paul Pu Liang, Zihao Deng, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ruslan Salakhutdinov, Louis-Philippe Morency' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2203.02013" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lvyiwei1/DIME" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The ability for a human to understand an Artificial Intelligence (AI) model’s decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="liang2021multibench" class="col-sm-8"> <div class="title">MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</div> <div class="author"> Paul Pu Liang, <em>Yiwei Lyu</em>, Xiang Fan, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Zetian Wu, Yun Cheng, Jason Wu, Leslie Yufan Chen, Peter Wu, Michelle A Lee, Yuke Zhu, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2107.07502.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/pliang279/MultiBench" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://cmu-multicomp-lab.github.io/multibench/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="lyu-etal-2021-styleptb" class="col-sm-8"> <div class="title">StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</div> <div class="author"> <em>Yiwei Lyu</em>, Paul Pu Liang, Hai Pham, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em> Jun 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2104.05196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lvyiwei1/StylePTB/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%69%77%65%69%6C%79%75%20%61%74%20%75%6D%69%63%68.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-3882-4246" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=Ibwd1swAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2066413750" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/lvyiwei1" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/yiwei-lyu-209176151" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/LyuYiwei1" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> - </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yiwei Lyu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: December 22, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>